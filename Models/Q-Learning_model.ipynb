{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0bf6f1c1e2c48a58f009f0e34ea057b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Button(description='Pos+', style=ButtonStyle()), Button(description='Pos-', stylâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "import os, sys, warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#sys.path.append('../Environment/')\n",
    "#from Environment.environment import *\n",
    "%run ../Environment/environment.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize some variables\n",
    "\n",
    "# Restart the environment\n",
    "env = EnvironmentState()\n",
    "\n",
    "NUM_ACTIONS = 5\n",
    "ACTIONS = np.asarray([0, 1, 2, 3, 4])\n",
    "\n",
    "NUM_POSITIONS = (env.REGION_HEIGHT/0.05)+1\n",
    "NUM_ANGLES    = ((258-102)/3)+1                               # Evironment states include a \n",
    "                                                              # range of angles (102 to 258)\n",
    "POS_STATES = np.zeros((int(NUM_POSITIONS), int(NUM_ANGLES)))  # for every y position the agent\n",
    "ANG_STATES = np.zeros((int(NUM_POSITIONS), int(NUM_ANGLES)))  # can occupy \n",
    "\n",
    "# Fill the state values\n",
    "for i in range(int(NUM_ANGLES)):\n",
    "    POS_STATES[:,i] = np.arange(-1*env.REGION_HEIGHT/2, env.REGION_HEIGHT/2+0.05, 0.05)\n",
    "    \n",
    "for i in range(int(NUM_POSITIONS)):\n",
    "    ANG_STATES[i,:] = np.arange(102, 258+3, 3)\n",
    "\n",
    "# Initialize the Q values\n",
    "Q = np.zeros((int(NUM_POSITIONS), int(NUM_ANGLES), int(NUM_ACTIONS)))\n",
    "\n",
    "# Resize the states and Q values for easier searching, intersections, and copmarisons\n",
    "POS_STATES = np.round(POS_STATES.reshape(int(NUM_POSITIONS*NUM_ANGLES), order = 'F'), 2)\n",
    "ANG_STATES = ANG_STATES.reshape(int(NUM_POSITIONS*NUM_ANGLES), order = 'F')\n",
    "Q = Q.reshape(int(NUM_POSITIONS*NUM_ANGLES), NUM_ACTIONS, order = 'F')\n",
    "\n",
    "# Now Q is a state x action matrix\n",
    "# Column 1 is wait action, 2 is pos up, 3 is pos down, 4 is aim up, 5 is aim down\n",
    "# Each row corresponds to a (pos & angle) state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize some variables\n",
    "\n",
    "# Restart the environment\n",
    "env = EnvironmentState()\n",
    "\n",
    "NUM_ACTIONS = 5 # Define the actions\n",
    "ACTIONS = np.asarray([0, 1, 2, 3, 4])\n",
    "\n",
    "# Define the state space, defined to be the possible y positions of te agent.\n",
    "NUM_STATES = (env.REGION_HEIGHT/0.05)+1\n",
    "STATES = np.round(np.arange(-1*env.REGION_HEIGHT/2, env.REGION_HEIGHT/2+0.05, 0.05),2)\n",
    "\n",
    "# Initialize the Q values\n",
    "Q = np.zeros((int(NUM_STATES), int(NUM_ACTIONS)))\n",
    "\n",
    "# Q is a state x action matrix\n",
    "# Column 1 is wait action, 2 is pos up, 3 is pos down, 4 is aim up, 5 is aim down\n",
    "# Each row corresponds to a pos state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 1.23599276e-01 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [1.23603262e-03 1.56175492e-01 0.00000000e+00 1.19095914e-03\n",
      "  1.22952789e-03]\n",
      " [1.36868086e-03 7.14048405e-02 1.92761474e-02 1.35919471e-03\n",
      "  1.44860402e-03]\n",
      " [2.89998841e-02 1.75807612e-03 5.14497998e-02 1.91292511e-03\n",
      "  4.38843048e-03]\n",
      " [1.90283531e-03 1.96125667e-03 1.81664551e-03 1.02623008e-01\n",
      "  1.93776568e-03]\n",
      " [2.18764056e-03 3.98551206e-03 2.21297519e-03 2.29290804e-03\n",
      "  2.24962317e-03]\n",
      " [2.60702162e-03 2.43798975e-02 2.49580694e-03 2.57286964e-03\n",
      "  2.56525914e-03]\n",
      " [3.17490200e-03 8.32547540e-02 3.21936693e-03 3.20746021e-03\n",
      "  3.03726123e-03]\n",
      " [3.78387702e-03 1.63306431e-01 4.09488846e-03 4.00011072e-03\n",
      "  0.00000000e+00]\n",
      " [5.33947294e-03 1.63802235e-01 5.44470426e-03 5.21048198e-03\n",
      "  4.96807480e-03]\n",
      " [1.10576895e-03 1.28153496e-01 6.43612806e-03 7.62447680e-03\n",
      "  0.00000000e+00]\n",
      " [1.02748839e-02 5.77889706e-02 7.32245037e-03 7.47291012e-03\n",
      "  7.72006480e-03]\n",
      " [1.09811897e-02 1.04754847e-02 1.04254284e-02 1.09848086e-02\n",
      "  1.09462200e-02]\n",
      " [0.00000000e+00 1.88919631e-03 0.00000000e+00 0.00000000e+00\n",
      "  2.17576086e-04]\n",
      " [2.83858627e-04 2.05801591e-01 2.73275449e-04 2.79328407e-04\n",
      "  2.83380806e-04]\n",
      " [3.81019944e-04 2.95685661e-01 3.69700640e-04 3.99621697e-04\n",
      "  3.98035202e-04]\n",
      " [4.36266616e-04 4.33028826e-04 3.07960465e-01 4.16813653e-04\n",
      "  4.40742956e-04]\n",
      " [3.28429632e-04 3.31326238e-04 2.57848498e-01 3.40226311e-04\n",
      "  3.37013548e-04]\n",
      " [0.00000000e+00 0.00000000e+00 3.43130368e-01 1.90798797e-04\n",
      "  2.98675620e-04]\n",
      " [0.00000000e+00 2.64540358e-02 4.90933674e-01 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [2.61328917e-02 2.58943855e-02 3.50480177e-01 2.69514995e-02\n",
      "  2.59606458e-02]\n",
      " [3.00640222e-03 4.36032557e-02 0.00000000e+00 4.04876952e-03\n",
      "  4.19544428e-03]\n",
      " [4.69276075e-03 1.06119970e-01 4.40260659e-03 4.25271231e-03\n",
      "  4.68242746e-03]\n",
      " [4.77640271e-03 1.51771897e-01 4.77968912e-03 4.47821875e-03\n",
      "  4.79113080e-03]\n",
      " [5.94385298e-03 2.85866092e-01 5.80214882e-03 2.07019639e-02\n",
      "  2.19257823e-02]\n",
      " [7.35709465e-03 4.66862491e-01 7.06779326e-03 7.36675324e-03\n",
      "  7.43595330e-03]\n",
      " [0.00000000e+00 2.64021647e-01 9.24483721e-03 9.72447659e-03\n",
      "  9.25350710e-03]\n",
      " [1.01746014e-02 2.11160216e-01 1.08983494e-02 3.69764313e-02\n",
      "  6.17861971e-03]\n",
      " [1.47704452e-02 8.98358282e-02 1.48198791e-02 1.39218459e-02\n",
      "  1.51039611e-02]\n",
      " [1.89667727e-02 5.14098874e-02 1.80303352e-02 1.91698633e-02\n",
      "  1.89237381e-02]\n",
      " [6.11363318e-04 1.97415147e-03 0.00000000e+00 1.71419228e-03\n",
      "  1.10153782e-03]\n",
      " [2.32246557e-03 2.25374082e-03 2.26769465e-03 2.16600171e-03\n",
      "  2.18731914e-03]\n",
      " [2.67344039e-03 3.07210666e-03 2.66608102e-03 2.68447559e-03\n",
      "  2.71040630e-03]\n",
      " [2.75730681e-03 2.67590117e-03 2.83695423e-03 2.74252623e-03\n",
      "  2.81324350e-03]\n",
      " [3.05616273e-03 4.01948580e-03 2.92857284e-03 3.03323929e-03\n",
      "  3.10976011e-03]\n",
      " [2.94288044e-03 2.90975724e-03 1.52517558e-02 2.83309950e-03\n",
      "  2.88303343e-03]\n",
      " [3.41097649e-03 2.34179009e-02 3.16596075e-03 3.86650864e-02\n",
      "  1.42455026e-01]\n",
      " [9.48882169e-03 3.41439547e-03 2.90715700e-01 3.44773021e-03\n",
      "  3.14430018e-03]\n",
      " [2.51121532e-03 0.00000000e+00 9.13217680e-02 0.00000000e+00\n",
      "  2.60372858e-03]\n",
      " [1.05891405e-02 0.00000000e+00 6.62917480e-02 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 3.20867432e-01 0.00000000e+00\n",
      "  0.00000000e+00]]\n",
      "[0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "0.062\n"
     ]
    }
   ],
   "source": [
    "# Initialize parameters\n",
    "NUM_OBS = 0 # Number of obstacles\n",
    "NUM_STEPS = 20 # Number of steps in an episode\n",
    "NUM_EPISODES = 500\n",
    "\n",
    "alpha = 0.2\n",
    "eps   = 0.01\n",
    "gamma = 0.5\n",
    "#episode = 0\n",
    "hits = np.zeros((NUM_EPISODES))\n",
    "\n",
    "# Use a Q-Learning algorithm based on the algorithm on page 131 of Sutton and Barto.\n",
    "for episode in range(NUM_EPISODES):\n",
    "    env.randomize(NUM_OBS)\n",
    "    n = 0\n",
    "    while True:\n",
    "        # Find the current state\n",
    "        #s = np.where( (POS_STATES == np.round(env._agent_position_y,2)) & (ANG_STATES == env._agent_aiming_angle) )[0]\n",
    "        s = np.where(STATES == np.round(env._agent_position_y,2))[0]\n",
    "        \n",
    "        # Create a policy based on epsilon greedy\n",
    "        if len(np.unique(Q[s,:])) <= 1: # If all Q values are same, set policy to equal for all actions\n",
    "            pi = np.zeros((NUM_ACTIONS))\n",
    "            pi[:] = 1/NUM_ACTIONS\n",
    "        elif len(np.unique(Q[s,:])) <= 4: # If multiple Q have a max, assign priority to the first max Q value\n",
    "            pi = np.asarray([((1 - eps) + eps/NUM_ACTIONS if Q[s,i]==np.max(Q[s,:]) else eps/NUM_ACTIONS) for i in range(NUM_ACTIONS)])\n",
    "            ind = np.where(pi == np.max(pi))[0]\n",
    "            pi[ind[1:]] = eps/NUM_ACTIONS\n",
    "        else:\n",
    "            pi = np.asarray([((1 - eps) + eps/NUM_ACTIONS if Q[s,i]==np.max(Q[s,:]) else eps/NUM_ACTIONS) for i in range(NUM_ACTIONS)])\n",
    "        \n",
    "        # Choose and take an action based on policy\n",
    "        action = np.random.choice(ACTIONS, replace = True, p = pi)\n",
    "        env.take_action(action)\n",
    "        \n",
    "        # With the action taken, the agent is now in the future state s'. Find that state\n",
    "        #s_prime = np.where( (POS_STATES == np.round(env._agent_position_y,2)) & (ANG_STATES == env._agent_aiming_angle) )[0]\n",
    "        s_prime = np.where(STATES == np.round(env._agent_position_y,2))[0]\n",
    "        \n",
    "        # Collect the reward\n",
    "        R = env.compute_reward()\n",
    "        \n",
    "        # Update the Q value\n",
    "        if len(np.unique(Q[s_prime,:])) <= 1: # If all Q values are the same, use any one\n",
    "            Q[s,action] = Q[s,action] + alpha*(R + gamma*Q[s_prime,0] - Q[s,action])\n",
    "        else:\n",
    "            Q[s,action] = Q[s,action] + alpha*(R + gamma*np.max(Q[s_prime,:]) - Q[s,action])\n",
    "        \n",
    "        n = n + 1\n",
    "        \n",
    "        # At the end of the episode, determine if the algorithm hit the target and break\n",
    "        # the loop to move on to the next episode\n",
    "        if n == NUM_STEPS:\n",
    "            hits[episode] = 1 if R == 1 else 0\n",
    "            break\n",
    "    \n",
    "print(Q)\n",
    "print(hits)\n",
    "print(np.sum(hits)/len(hits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5 183\n",
      "[0.02 0.02 0.92 0.02 0.92] 0.1\n",
      "[-1.   -0.95 -0.9  -0.85 -0.8  -0.75 -0.7  -0.65 -0.6  -0.55 -0.5  -0.45\n",
      " -0.4  -0.35 -0.3  -0.25 -0.2  -0.15 -0.1  -0.05  0.    0.05  0.1   0.15\n",
      "  0.2   0.25  0.3   0.35  0.4   0.45  0.5   0.55  0.6   0.65  0.7   0.75\n",
      "  0.8   0.85  0.9   0.95  1.  ]\n"
     ]
    }
   ],
   "source": [
    "print(round(env._agent_position_y,2), env._agent_aiming_angle)\n",
    "\n",
    "print(pi, eps)\n",
    "print(env.Y_POSITIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
