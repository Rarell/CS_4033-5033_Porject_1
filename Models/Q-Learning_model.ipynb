{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c6b99f1574445b8bd2203764eff8cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Button(description='Pos+', style=ButtonStyle()), Button(description='Pos-', stylâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "import os, sys, warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#sys.path.append('../Environment/')\n",
    "#from Environment.environment import *\n",
    "%run ../Environment/environment.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize some variables\n",
    "\n",
    "# Restart the environment\n",
    "env = EnvironmentState()\n",
    "\n",
    "NUM_ACTIONS = 5\n",
    "ACTIONS = np.asarray([0, 1, 2, 3, 4])\n",
    "\n",
    "NUM_POSITIONS = (env.REGION_HEIGHT/0.05)+1\n",
    "NUM_ANGLES    = ((258-102)/3)+1                               # Evironment states include a \n",
    "                                                              # range of angles (102 to 258)\n",
    "POS_STATES = np.zeros((int(NUM_POSITIONS), int(NUM_ANGLES)))  # for every y position the agent\n",
    "ANG_STATES = np.zeros((int(NUM_POSITIONS), int(NUM_ANGLES)))  # can occupy \n",
    "\n",
    "# Fill the state values\n",
    "for i in range(int(NUM_ANGLES)):\n",
    "    POS_STATES[:,i] = np.arange(-1*env.REGION_HEIGHT/2, env.REGION_HEIGHT/2+0.05, 0.05)\n",
    "    \n",
    "for i in range(int(NUM_POSITIONS)):\n",
    "    ANG_STATES[i,:] = np.arange(102, 258+3, 3)\n",
    "\n",
    "# Initialize the Q values\n",
    "Q = np.zeros((int(NUM_POSITIONS), int(NUM_ANGLES), int(NUM_ACTIONS)))\n",
    "\n",
    "# Resize the states and Q values for easier searching, intersections, and copmarisons\n",
    "POS_STATES = np.round(POS_STATES.reshape(int(NUM_POSITIONS*NUM_ANGLES), order = 'F'), 2)\n",
    "ANG_STATES = ANG_STATES.reshape(int(NUM_POSITIONS*NUM_ANGLES), order = 'F')\n",
    "Q = Q.reshape(int(NUM_POSITIONS*NUM_ANGLES), NUM_ACTIONS, order = 'F')\n",
    "\n",
    "# Now Q is a state x action matrix\n",
    "# Column 1 is wait action, 2 is pos up, 3 is pos down, 4 is aim up, 5 is aim down\n",
    "# Each row corresponds to a (pos & angle) state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize some variables\n",
    "\n",
    "# Restart the environment\n",
    "env = EnvironmentState()\n",
    "\n",
    "NUM_ACTIONS = 5 # Define the actions\n",
    "ACTIONS = np.asarray([0, 1, 2, 3, 4])\n",
    "\n",
    "# Define the state space, defined to be the possible y positions of te agent.\n",
    "NUM_STATES = (env.REGION_HEIGHT/0.05)+1\n",
    "STATES = np.round(np.arange(-1*env.REGION_HEIGHT/2, env.REGION_HEIGHT/2+0.05, 0.05),2)\n",
    "\n",
    "# Initialize the Q values\n",
    "Q = np.zeros((int(NUM_STATES), int(NUM_ACTIONS)))\n",
    "\n",
    "# Q is a state x action matrix\n",
    "# Column 1 is wait action, 2 is pos up, 3 is pos down, 4 is aim up, 5 is aim down\n",
    "# Each row corresponds to a pos state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0.]\n",
      "0.096\n"
     ]
    }
   ],
   "source": [
    "# Initialize parameters\n",
    "NUM_OBS = 0 # Number of obstacles\n",
    "NUM_STEPS = 100 # Number of steps in an episode\n",
    "NUM_EPISODES = 500\n",
    "\n",
    "alpha = 0.2\n",
    "eps   = 0.01\n",
    "gamma = 0.5\n",
    "#episode = 0\n",
    "hits = np.zeros((NUM_EPISODES))\n",
    "\n",
    "# Use a Q-Learning algorithm based on the algorithm on page 131 of Sutton and Barto.\n",
    "for episode in range(NUM_EPISODES):\n",
    "    env.initialize() # Try different levels of complexity for agent and target position\n",
    "    #env.randomize_agent()\n",
    "    env.randomize_target()\n",
    "    #env.randomize(NUM_OBS)\n",
    "    n = 0\n",
    "    while True:\n",
    "        # Find the current state\n",
    "        #s = np.where( (POS_STATES == np.round(env._agent_position_y,2)) & (ANG_STATES == env._agent_aiming_angle) )[0]\n",
    "        s = np.where(STATES == np.round(env._agent_position_y,2))[0]\n",
    "        \n",
    "        # Create a policy based on epsilon greedy\n",
    "        if len(np.unique(Q[s,:])) <= 1: # If all Q values are same, set policy to equal for all actions\n",
    "            pi = np.zeros((NUM_ACTIONS))\n",
    "            pi[:] = 1/NUM_ACTIONS\n",
    "        elif len(np.unique(Q[s,:])) <= 4: # If multiple Q have a max, assign priority to the first max Q value\n",
    "            pi = np.asarray([((1 - eps) + eps/NUM_ACTIONS if Q[s,i]==np.max(Q[s,:]) else eps/NUM_ACTIONS) for i in range(NUM_ACTIONS)])\n",
    "            ind = np.where(pi == np.max(pi))[0]\n",
    "            pi[ind[1:]] = eps/NUM_ACTIONS\n",
    "        else:\n",
    "            pi = np.asarray([((1 - eps) + eps/NUM_ACTIONS if Q[s,i]==np.max(Q[s,:]) else eps/NUM_ACTIONS) for i in range(NUM_ACTIONS)])\n",
    "        \n",
    "        # Choose and take an action based on policy\n",
    "        action = np.random.choice(ACTIONS, replace = True, p = pi)\n",
    "        env.take_action(action)\n",
    "        \n",
    "        # With the action taken, the agent is now in the future state s'. Find that state\n",
    "        #s_prime = np.where( (POS_STATES == np.round(env._agent_position_y,2)) & (ANG_STATES == env._agent_aiming_angle) )[0]\n",
    "        s_prime = np.where(STATES == np.round(env._agent_position_y,2))[0]\n",
    "        \n",
    "        # Collect the reward\n",
    "        R = env.compute_reward()\n",
    "        \n",
    "        # Update the Q value\n",
    "        if len(np.unique(Q[s_prime,:])) <= 1: # If all Q values are the same, use any one\n",
    "            Q[s,action] = Q[s,action] + alpha*(R + gamma*Q[s_prime,0] - Q[s,action])\n",
    "        else:\n",
    "            Q[s,action] = Q[s,action] + alpha*(R + gamma*np.max(Q[s_prime,:]) - Q[s,action])\n",
    "        \n",
    "        n = n + 1\n",
    "        \n",
    "        # At the end of the episode, determine if the algorithm hit the target and break\n",
    "        # the loop to move on to the next episode\n",
    "        if n == NUM_STEPS:\n",
    "            hits[episode] = 1 if R == 1 else 0\n",
    "            break\n",
    "\n",
    "print(hits)\n",
    "print(np.sum(hits)/len(hits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7 261\n",
      "[0.002 0.002 0.002 0.002 0.992] 0.01\n",
      "[-1.   -0.95 -0.9  -0.85 -0.8  -0.75 -0.7  -0.65 -0.6  -0.55 -0.5  -0.45\n",
      " -0.4  -0.35 -0.3  -0.25 -0.2  -0.15 -0.1  -0.05  0.    0.05  0.1   0.15\n",
      "  0.2   0.25  0.3   0.35  0.4   0.45  0.5   0.55  0.6   0.65  0.7   0.75\n",
      "  0.8   0.85  0.9   0.95  1.  ]\n"
     ]
    }
   ],
   "source": [
    "print(round(env._agent_position_y,2), env._agent_aiming_angle)\n",
    "\n",
    "print(action)\n",
    "print(env.Y_POSITIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
