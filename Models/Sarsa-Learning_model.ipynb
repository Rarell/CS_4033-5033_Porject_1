{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ab2c1b67e84c5b81343c4d9bdfdb06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Button(description='Pos+', style=ButtonStyle()), Button(description='Pos-', stylâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import libraries\n",
    "\n",
    "import os, sys, warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "#sys.path.append('../Environment/')\n",
    "#from Environment.environment import *\n",
    "%run ../Environment/environment.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_matrix(name, matrix):\n",
    "    matrix = np.atleast_2d(matrix)\n",
    "    s = ''\n",
    "    for idx, v in enumerate(matrix.flatten()):\n",
    "        s = s + ' {:0.3f} '.format(v)\n",
    "        if idx != (matrix.size-1):\n",
    "            if idx % matrix.shape[0] == matrix.shape[0]-1:\n",
    "                s = s + '\\\\\\\\'\n",
    "            else:\n",
    "                s = s + '&'\n",
    "    return md(name + ' = $' + '\\\\begin{bmatrix}' + s + '\\\\end{bmatrix}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Environment / Model Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart the environment\n",
    "env = EnvironmentState()\n",
    "\n",
    "# Define the actions\n",
    "ACTIONS = np.asarray([0, 1, 2, 3, 4])\n",
    "NUM_ACTIONS = len(ACTIONS)\n",
    "\n",
    "# Define the state space, defined to be the possible y positions of te agent.\n",
    "NUM_POS_STATES = (env.REGION_HEIGHT/0.05)+1\n",
    "NUM_ANG_STATES = (160/3)+1\n",
    "POS_STATES = np.linspace(0, env.REGION_HEIGHT, int(NUM_POS_STATES)) - env.REGION_HEIGHT/2\n",
    "ANG_STATES = np.round(np.round(np.linspace(100, 260, int(NUM_ANG_STATES))))\n",
    "\n",
    "NUM_ANG_STATES = 1\n",
    "ANG_STATES = np.array([0])\n",
    "\n",
    "# Initialize state-value function\n",
    "Q = np.zeros((int(NUM_POS_STATES), int(NUM_ANG_STATES), int(NUM_ACTIONS)))\n",
    "\n",
    "# Q is a state x action matrix\n",
    "# Column 1 is wait action, 2 is pos up, 3 is pos down, 4 is aim up, 5 is aim down\n",
    "# Each row corresponds to a pos state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Sarsa Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "NUM_OBS = 0 # Number of obstacles\n",
    "NUM_STEPS = 10 # Number of steps in an episode\n",
    "NUM_EPISODES = 100\n",
    "\n",
    "alpha = 0.02\n",
    "eps   = 0.01\n",
    "gamma = 0.90\n",
    "\n",
    "hits  = np.zeros((NUM_EPISODES))\n",
    "R_SUM = np.zeros((NUM_EPISODES))\n",
    "MSE   = np.zeros((NUM_EPISODES))\n",
    "\n",
    "def action_from_epsilon_greedy(epsilon, Q_at_state):\n",
    "    Q_at_state = np.squeeze(Q_at_state)\n",
    "    # Create a policy based on epsilon greedy\n",
    "    if len(np.unique(Q_at_state)) <= 1: # If all Q values are same, set policy to equal for all actions\n",
    "        pi = np.zeros((NUM_ACTIONS))\n",
    "        pi[:] = 1/NUM_ACTIONS\n",
    "    elif len(np.unique(Q_at_state)) <= 4: # If multiple Q have a max, assign priority to the first max Q value\n",
    "        pi = np.asarray([((1 - eps) + eps/NUM_ACTIONS if Q_at_state[i]==np.max(Q_at_state) else eps/NUM_ACTIONS) for i in range(NUM_ACTIONS)])\n",
    "        ind = np.where(pi == np.max(pi))[0]\n",
    "        pi[ind[1:]] = eps/NUM_ACTIONS\n",
    "    else:\n",
    "        pi = np.asarray([((1 - eps) + eps/NUM_ACTIONS if Q_at_state[i]==np.max(Q_at_state) else eps/NUM_ACTIONS) for i in range(NUM_ACTIONS)])\n",
    "    return np.random.choice(ACTIONS, replace = True, p = pi)\n",
    "\n",
    "def get_env_state(env):\n",
    "    pos = np.argmin(np.abs(POS_STATES - env._agent_position_y))\n",
    "    ang = np.argmin(np.abs(ANG_STATES - env._agent_aiming_angle))\n",
    "    return pos, ang\n",
    "\n",
    "# Sarsa-learning model\n",
    "for episode in range(NUM_EPISODES):\n",
    "    # Try different levels of complexity for agent and target position\n",
    "    env.initialize() \n",
    "    env.centered_obstruction()\n",
    "    \n",
    "    # Choose action based on epsilon-greedy policy\n",
    "    a = action_from_epsilon_greedy(eps, Q[sp, 0, :])\n",
    "    \n",
    "    R_vector = np.zeros((NUM_STEPS))\n",
    "    n = 0\n",
    "    while True:\n",
    "        # Find the current state\n",
    "        sp, sa = get_env_state(env)\n",
    "        \n",
    "        # Take action\n",
    "        env.take_action(a)\n",
    "        \n",
    "        # With the action taken, the agent is now in the future state s'. Find that state\n",
    "        sp_prime, sa_prime = get_env_state(env)\n",
    "        # choose a_prime using epsilon greedy policy \n",
    "        a_prime = action_from_epsilon_greedy(eps, Q[sp_prime, 0, :])\n",
    "        \n",
    "        # Collect the reward\n",
    "        R = env.compute_reward()\n",
    "        R_vector[n] = R\n",
    "        \n",
    "        # Update Q value\n",
    "        Q[sp, 0, a] = Q[sp, 0, a] + alpha*(R + gamma*Q[sp_prime, 0, a_prime] - Q[sp, 0, a])\n",
    "        \n",
    "        # use as next action\n",
    "        a = a_prime\n",
    "        \n",
    "        n = n + 1\n",
    "        # At the end of the episode, determine if the algorithm hit the target and break\n",
    "        # the loop to move on to the next episode\n",
    "        if n == NUM_STEPS:\n",
    "            hits[episode] = 1 if R == 1 else 0 # If agent is aiming at the target, R is 1. \n",
    "            R_SUM[episode] = np.sum(R_vector)\n",
    "            MSE[episode]  = np.var(R_vector) # For the unbiased case, the RSE is the variance\n",
    "            break\n",
    "\n",
    "# print(hits)\n",
    "print(np.sum(hits)/len(hits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Example graphs to make:\n",
    "# Sum of rewards vs. Episodes\n",
    "# % accuracy vs. Episodes\n",
    "# RMSE vs. Episodes\n",
    "plt.figure()\n",
    "episodes = np.arange(1, NUM_EPISODES+1, 1)\n",
    "\n",
    "R_Sum_Ave = np.zeros((NUM_EPISODES))\n",
    "accuracy = np.zeros((NUM_EPISODES))\n",
    "MSE_Ave = np.zeros((NUM_EPISODES))\n",
    "\n",
    "R_Sum_Ave[0] = R_SUM[0]\n",
    "R_Sum_Ave[1:] = np.asarray([np.sum(R_SUM[:n])/(n+1) for n in range(1, NUM_EPISODES)])\n",
    "\n",
    "accuracy[0] = hits[0]\n",
    "accuracy[1:] = np.asarray([np.sum(hits[:n])/(n+1) for n in range(1, NUM_EPISODES)])\n",
    "\n",
    "MSE_Ave[0] = MSE[0]\n",
    "MSE_Ave[1:] = np.asarray([np.sum(MSE[:n])/(n+1) for n in range(1, NUM_EPISODES)])\n",
    "\n",
    "# Make an example graph\n",
    "fig, axes = plt.subplots(figsize = [12,18], nrows = 3, ncols = 1, sharex = True)\n",
    "ax1 = axes[0]; ax2 = axes[1]; ax3 = axes[2]\n",
    "\n",
    "ax1.plot(episodes, R_Sum_Ave, 'r-')\n",
    "ax1.set_ylabel('Average Sum of Rewards', fontsize = 14)\n",
    "\n",
    "ax2.plot(episodes, accuracy, 'r-')\n",
    "ax2.set_ylabel('Average Accuracy', fontsize = 14)\n",
    "\n",
    "ax3.plot(episodes, MSE_Ave, 'r-')\n",
    "ax3.set_ylabel('Mean Squared Error of Reward')\n",
    "ax3.set_xlabel('Episodes', fontsize = 14)\n",
    "\n",
    "plt.show(block = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the data to a comma seperated text file for comparison with another algorithm.\n",
    "# Naming format:\n",
    "# Q-Learning_mo_nr.txt\n",
    "# mo refers to the number of obstacles used when the model learned\n",
    "#   (e.g., 0o is 0 obstacles, 1o is 1 obstacle, etc.)\n",
    "# nr will be filled with:\n",
    "#   nr: No Randomized (neither target nor agent were randomized per episode)\n",
    "#   tr: Target Randomized (only the target was randomized per episode)\n",
    "#   ar: Agent Randomized (only the agent was randomized per episode)\n",
    "#   br: Both Randomized (both the target and agent were randomized per episode)\n",
    "\n",
    "# Variables saved are:\n",
    "#   Episodes\n",
    "#   Hit/Miss for that episode\n",
    "#   Sum of Rewards for that episode (SR)\n",
    "#   Mean Square Error (Variance) for that episode (MSE)\n",
    "\n",
    "# Write the file\n",
    "f = open('Q-Learning_0o_tr.txt', 'w')\n",
    "\n",
    "# Write the headers\n",
    "f.write('Episode,Hit,SR,MSE' + '\\n')\n",
    "\n",
    "# Write the data\n",
    "for n in range(NUM_EPISODES):\n",
    "    f.write(str(episodes[n]) + ',' + str(hits[n]) + ',' + str(R_SUM) + ',' + str(MSE) + '\\n')\n",
    "    \n",
    "# Close the file when finished\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
